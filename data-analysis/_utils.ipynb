{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.polynomial.polynomial as poly\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', 'This pattern has match groups')\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import zscore, wilcoxon, mannwhitneyu\n",
    "tqdm_notebook.pandas()\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "PROJECT_ROOT = '..'\n",
    "CSV_FOLDER = 'csv'\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT, 'images')\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "GK_GREEN = '#21B534'\n",
    "GK_GREEN_LIGHT = '#07f596'\n",
    "GK_PURPLE = '#7494EA'\n",
    "GK_BLUE = '#19535F'\n",
    "RED = \"#f55b5b\"\n",
    "DARK_RED = \"#db0000\"\n",
    "GREY = \"#a6a6a6\"\n",
    "BLUE = \"#4287f5\"\n",
    "LIGHT_GREEN = '#88D39B'\n",
    "\n",
    "MAJOR = 'MAJOR'\n",
    "MINOR = 'MINOR'\n",
    "PATCH = 'PATCH'\n",
    "PINNED = 'PINNED'\n",
    "NA_RELEASE_TYPE = pd.NA\n",
    "DEP = 'Dependency'\n",
    "DEV_DEP = 'Dev Dependency'\n",
    "\n",
    "COLOUR_PALETTE = sns.color_palette('Greens_r')\n",
    "GK_GREEN = COLOUR_PALETTE[1]\n",
    "TWO_COLOUR_PALETTE = [GK_GREEN, GK_PURPLE]\n",
    "THREE_COLOUR_PALETTE = [GK_GREEN, GK_PURPLE, GK_BLUE]\n",
    "FOUR_COLOUR_PALETTE = THREE_COLOUR_PALETTE + [LIGHT_GREEN]\n",
    "\n",
    "BINS_COUNT = 40\n",
    "\n",
    "PLOT_LABEL_SIZE = 16\n",
    "PLOT_TICK_LABEL_SIZE = 14\n",
    "\n",
    "FIG_SIZE_W = 7.5\n",
    "FIF_SIZE_H = 6\n",
    "\n",
    "%run _cliffs_delta.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_issues():\n",
    "    dtypes = {\n",
    "        'issue_id': 'int64',\n",
    "        'issue_number': 'int64',\n",
    "        'issue_url': 'object',\n",
    "        'issue_title': 'object',\n",
    "        'issue_state': 'category',\n",
    "        'issue_is_locked': 'bool',\n",
    "        'issue_created_at': 'object',\n",
    "        'issue_updated_at': 'object',\n",
    "        'issue_closed_at': 'object',\n",
    "        'issue_user_login': 'category',\n",
    "        'issue_labels': 'category',\n",
    "        'issue_num_comments': 'int64',\n",
    "        'issue_events_url': 'object',\n",
    "        'issue_dependency_name': 'category',\n",
    "        'issue_dependency_type': 'category',\n",
    "        'issue_dependency_actual_version': 'object',\n",
    "        'issue_dependency_next_version': 'object',\n",
    "        'issue_dependency_bundle_name': 'category',\n",
    "        'issue_body_parser': 'category',\n",
    "        'issue_repo_url': 'object',\n",
    "        'update_type': 'category',\n",
    "    }\n",
    "    parse_dates = ['issue_created_at', 'issue_updated_at', 'issue_closed_at']\n",
    "    renaming = {\n",
    "        'issue_id': 'id',\n",
    "        'issue_number': 'number',\n",
    "        'issue_url': 'url',\n",
    "        'issue_title': 'title',\n",
    "        'issue_state': 'state',\n",
    "        'issue_is_locked': 'is_locked',\n",
    "        'issue_created_at': 'created_at',\n",
    "        'issue_updated_at': 'updated_at',\n",
    "        'issue_closed_at': 'closed_at',\n",
    "        'issue_user_login': 'user_login',\n",
    "        'issue_labels': 'labels',\n",
    "        'issue_num_comments': 'num_comments',\n",
    "        'issue_events_url': 'events_url',\n",
    "        'issue_dependency_name': 'dependency_name',\n",
    "        'issue_dependency_type': 'dependency_type',\n",
    "        'issue_dependency_actual_version': 'dependency_actual_version',\n",
    "        'issue_dependency_next_version': 'dependency_next_version',\n",
    "        'issue_dependency_bundle_name': 'dependency_bundle_name',\n",
    "        'issue_body_parser': 'body_parser',\n",
    "        'issue_repo_url': 'repo_url',\n",
    "        'update_type': 'update_type',\n",
    "    }\n",
    "    file_path = f'{PROJECT_ROOT}/{CSV_FOLDER}/aug_greenkeeper_issues.csv'\n",
    "    issues = pd.read_csv(\n",
    "        file_path,\n",
    "        dtype=dtypes,\n",
    "        parse_dates=parse_dates\n",
    "    ).rename(columns=renaming)\n",
    "    issues['repo_name'] = issues['repo_url'].apply(lambda s: s.split('https://api.github.com/repos/')[1])\n",
    "    return issues\n",
    "\n",
    "def load_events():\n",
    "    events = pd.read_csv(f'{PROJECT_ROOT}/{CSV_FOLDER}/greenkeeper_events.csv')\n",
    "    events['event_description'] = events['event_description'].astype('category')\n",
    "    return events.copy()\n",
    "\n",
    "def load_comments():\n",
    "    comments = pd.read_csv(f'{PROJECT_ROOT}/{CSV_FOLDER}/greenkeeper_comments.csv')\n",
    "    comments['comment_created_at'] = pd.to_datetime(comments['comment_created_at'])\n",
    "    comments['comment_updated_at'] = pd.to_datetime(comments['comment_updated_at'])\n",
    "    comments['comment_user_type'] = comments['comment_user_type'].astype('category')\n",
    "    return comments.copy()\n",
    "    \n",
    "def load_commits():\n",
    "    dtypes = {\n",
    "        'commit_event_url': 'object',\n",
    "        'commit_event_id': 'int64',\n",
    "        'commit_message': 'object',\n",
    "        'commit_git_committer_email': 'object',\n",
    "        'commit_git_committer_name': 'object',\n",
    "        'commit_git_author_email': 'object',\n",
    "        'commit_git_author_name': 'object',\n",
    "        'commit_github_committer_login': 'object',\n",
    "        'commit_github_committer_id': 'float64',\n",
    "        'commit_github_committer_type': 'category',\n",
    "        'commit_github_author_login': 'object',\n",
    "        'commit_github_author_id': 'float64',\n",
    "        'commit_github_author_type': 'category',\n",
    "        'commit_stats_deletions': 'int64',\n",
    "        'commit_stats_additions': 'int64',\n",
    "        'commit_stats_total': 'int64',\n",
    "        'commit_tree_sha': 'object',\n",
    "        'commit_sha': 'object',\n",
    "        'commit_num_parents': 'int64',\n",
    "        'commit_num_comments': 'int64',\n",
    "        'commit_file_name': 'category',\n",
    "        'commit_file_additions': 'int64',\n",
    "        'commit_file_deletions': 'int64',\n",
    "        'commit_file_changes': 'int64',\n",
    "        'commit_file_sha': 'object',\n",
    "        'commit_file_status': 'category',\n",
    "        'commit_issue_id': 'float64',\n",
    "    }\n",
    "    renaming = {\n",
    "        'commit_event_url': 'event_url',\n",
    "        'commit_event_id': 'event_id',\n",
    "        'commit_message': 'message',\n",
    "        'commit_git_committer_email': 'committer_email',\n",
    "        'commit_git_committer_name': 'committer_name',\n",
    "        'commit_git_author_email': 'author_email',\n",
    "        'commit_git_author_name': 'author_email',\n",
    "        'commit_github_committer_login': 'committer_login',\n",
    "        'commit_github_committer_id': 'committer_id',\n",
    "        'commit_github_committer_type': 'committer_type',\n",
    "        'commit_github_author_login': 'author_login',\n",
    "        'commit_github_author_id': 'author_id',\n",
    "        'commit_github_author_type': 'author_type',\n",
    "        'commit_stats_deletions': 'deletions',\n",
    "        'commit_stats_additions': 'additions',\n",
    "        'commit_stats_total': 'stats_total',\n",
    "        'commit_tree_sha': 'tree_sha',\n",
    "        'commit_sha': 'sha',\n",
    "        'commit_num_parents': 'num_parents',\n",
    "        'commit_num_comments': 'num_comments',\n",
    "        'commit_file_name': 'file_name',\n",
    "        'commit_file_additions': 'file_additions',\n",
    "        'commit_file_deletions': 'file_deletions',\n",
    "        'commit_file_changes': 'file_changes',\n",
    "        'commit_file_sha': 'file_sha',\n",
    "        'commit_file_status': 'file_status',\n",
    "        'commit_issue_id': 'issue_id',\n",
    "    }\n",
    "    file_path = f'{PROJECT_ROOT}/{CSV_FOLDER}/aug_greenkeeper_commits.csv'\n",
    "    commits = pd.read_csv(\n",
    "        file_path,\n",
    "        dtype=dtypes,\n",
    "     ).rename(columns=renaming)\n",
    "    return commits\n",
    "\n",
    "def load_package_names():\n",
    "    dtypes = {\n",
    "        'package_name': 'category',\n",
    "        'package_gh_url_api': 'object',\n",
    "        'package_gh_url': 'object',\n",
    "        'package_author': 'category',\n",
    "        'package_description': 'object',\n",
    "        'package_repo_url': 'object',\n",
    "        'package_repo_type': 'category',\n",
    "        'package_version': 'object',\n",
    "        'package_dependencies': 'object',\n",
    "        'package_dev_dependencies': 'object',\n",
    "        'package_peer_dependencies': 'object',\n",
    "    }\n",
    "    renaming = {\n",
    "        'package_name': 'name',\n",
    "        'package_gh_url_api': 'gh_url_api',\n",
    "        'package_gh_url': 'gh_url',\n",
    "        'package_author': 'author',\n",
    "        'package_description': 'description',\n",
    "        'package_repo_url': 'repo_url',\n",
    "        'package_repo_type': 'repo_type',\n",
    "        'package_version': 'version',\n",
    "        'package_dependencies': 'dependencies',\n",
    "        'package_dev_dependencies': 'dev_dependencies',\n",
    "        'package_peer_dependencies': 'peer_dependencies',\n",
    "    }\n",
    "    file_path = f'{PROJECT_ROOT}/{CSV_FOLDER}/greenkeeper_package_names.csv'\n",
    "    package_names = pd.read_csv(\n",
    "        file_path,\n",
    "        dtype=dtypes\n",
    "    ).rename(columns=renaming)\n",
    "    return package_names\n",
    "\n",
    "def load_library_versions():\n",
    "    dtypes = {\n",
    "        'package_name': 'category',\n",
    "        'version': 'object',\n",
    "        'version_published_at': 'object',\n",
    "        'version_release_type': 'category',\n",
    "        'broken_clients_count': 'int64',\n",
    "        'time_until_next_release': 'object',\n",
    "    }\n",
    "    parse_dates = ['version_published_at']\n",
    "    file_path = f'{PROJECT_ROOT}/{CSV_FOLDER}/aug_breaking_library_versions.csv'\n",
    "    library_versions = pd.read_csv(\n",
    "        file_path,\n",
    "        dtype=dtypes,\n",
    "        parse_dates=parse_dates,\n",
    "    )\n",
    "    library_versions[\"time_until_next_release\"] = pd.to_timedelta(library_versions[\"time_until_next_release\"])\n",
    "    return library_versions\n",
    "\n",
    "def load_library_releases():\n",
    "    library_releases = pd.read_csv(f'{PROJECT_ROOT}/{CSV_FOLDER}/breaking_library_releases.csv')\n",
    "    library_releases[\"first_release_date\"] = library_releases[\"first_release_date\"].astype(\"datetime64\")\n",
    "    library_releases[\"last_release_date\"] = library_releases[\"last_release_date\"].astype(\"datetime64\")\n",
    "    library_releases[\"avg_time_between_releases\"] = \\\n",
    "        pd.to_timedelta(library_releases[\"avg_time_between_releases\"])\n",
    "    return library_releases.copy()\n",
    "\n",
    "def load_package_releases_and_breaks():\n",
    "    package_releases_and_breaks = \\\n",
    "        pd.read_csv(f'{PROJECT_ROOT}/{CSV_FOLDER}/releases_and_breaks_counts_by_package.csv')\n",
    "    return package_releases_and_breaks.copy()\n",
    "\n",
    "def load_provider_to_clients():\n",
    "    provider_to_clients = \\\n",
    "        pd.read_csv(f'{PROJECT_ROOT}/{CSV_FOLDER}/provider_to_clients.csv')\n",
    "    return provider_to_clients.copy()\n",
    "\n",
    "def load_package_dependencies():\n",
    "    package_dependencies = \\\n",
    "        pd.read_csv(f'{PROJECT_ROOT}/{CSV_FOLDER}/package_dependencies.csv')\n",
    "    return package_dependencies.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_issues():\n",
    "    dtypes = {\n",
    "        'id': 'int64',\n",
    "        'repo_name': 'category',\n",
    "        'url': 'object',\n",
    "        'repository_url': 'object',\n",
    "        'comments_url': 'object',\n",
    "        'events_url': 'object',\n",
    "        'html_url': 'object',\n",
    "        'number': 'int64',\n",
    "        'title': 'object',\n",
    "        'user_id': 'int64',\n",
    "        'user_login': 'category',\n",
    "        'user_type': 'category',\n",
    "        'state': 'category',\n",
    "        'locked': 'bool',\n",
    "        'comments': 'int64',\n",
    "        'created_at': 'object',\n",
    "        'updated_at': 'object',\n",
    "        'closed_at': 'object',\n",
    "        'body': 'object',\n",
    "        'is_pull_request': 'bool',\n",
    "    }\n",
    "    parse_dates = ['created_at', 'updated_at', 'closed_at']\n",
    "    file_path = f'{PROJECT_ROOT}/{CSV_FOLDER}/all_project_issues.csv'\n",
    "    all_issues = pd.read_csv(\n",
    "        file_path,\n",
    "        dtype=dtypes,\n",
    "        parse_dates=parse_dates,\n",
    "    )\n",
    "    return all_issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_non_gk_issues_for_analysis():\n",
    "    dtypes = {\n",
    "        'id': 'int64',\n",
    "        'repo_name': 'category',\n",
    "        'url': 'object',\n",
    "        'repository_url': 'object',\n",
    "        'comments_url': 'object',\n",
    "        'events_url': 'object',\n",
    "        'html_url': 'object',\n",
    "        'number': 'int64',\n",
    "        'title': 'object',\n",
    "        'user_id': 'int64',\n",
    "        'user_login': 'category',\n",
    "        'user_type': 'category',\n",
    "        'state': 'category',\n",
    "        'locked': 'bool',\n",
    "        'comments': 'int64',\n",
    "        'created_at': 'object',\n",
    "        'updated_at': 'object',\n",
    "        'closed_at': 'object',\n",
    "        'body': 'object',\n",
    "        'is_pull_request': 'bool',\n",
    "    }\n",
    "    parse_dates = ['created_at', 'updated_at', 'closed_at']\n",
    "    file_path = f'{PROJECT_ROOT}/{CSV_FOLDER}/non_gk_issues_for_analysis.csv'\n",
    "    issues = pd.read_csv(\n",
    "        file_path,\n",
    "        dtype=dtypes,\n",
    "        parse_dates=parse_dates,\n",
    "    )\n",
    "    return issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gk_issues_for_analysis():\n",
    "    dtypes = {\n",
    "        'id': 'int64',\n",
    "        'number': 'int64',\n",
    "        'url': 'object',\n",
    "        'title': 'object',\n",
    "        'state': 'category',\n",
    "        'is_locked': 'bool',\n",
    "        'created_at': 'object',\n",
    "        'updated_at': 'object',\n",
    "        'closed_at': 'object',\n",
    "        'user_login': 'category',\n",
    "        'labels': 'category',\n",
    "        'num_comments': 'int64',\n",
    "        'events_url': 'object',\n",
    "        'dependency_name': 'category',\n",
    "        'dependency_type': 'object',\n",
    "        'dependency_actual_version': 'object',\n",
    "        'dependency_next_version': 'object',\n",
    "        'dependency_bundle_name': 'category',\n",
    "        'body_parser': 'category',\n",
    "        'repo_url': 'object',\n",
    "        'update_type': 'category',\n",
    "        'repo_name': 'object',\n",
    "        'html_url': 'object',\n",
    "        'body': 'object',\n",
    "    }\n",
    "    parse_dates = ['created_at', 'updated_at', 'closed_at']\n",
    "    file_path = f'{PROJECT_ROOT}/{CSV_FOLDER}/gk_issues_for_analysis.csv'\n",
    "    issues = pd.read_csv(\n",
    "        file_path,\n",
    "        dtype=dtypes,\n",
    "        parse_dates=parse_dates,\n",
    "    )\n",
    "    return issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_non_gkirbbi_comments():\n",
    "    dtypes = {\n",
    "        'id': 'int64',\n",
    "        'issue_id': 'int64',\n",
    "        'repo_name': 'category',\n",
    "        'url': 'object',\n",
    "        'issue_url': 'object',\n",
    "        'user_id': 'int64',\n",
    "        'user_login': 'category',\n",
    "        'user_type': 'category',\n",
    "        'created_at': 'object',\n",
    "        'updated_at': 'object',\n",
    "        'body': 'object',\n",
    "    }\n",
    "    parse_dates = ['created_at', 'updated_at']\n",
    "    file_path = f'{PROJECT_ROOT}/{CSV_FOLDER}/non_gkirbbi_project_issue_comments.csv'\n",
    "    non_gkirbbi_comments = pd.read_csv(\n",
    "        file_path,\n",
    "        dtype=dtypes,\n",
    "        parse_dates=parse_dates,\n",
    "    )\n",
    "    return non_gkirbbi_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bens_collected_commits():\n",
    "    file_path = f'{PROJECT_ROOT}/{CSV_FOLDER}/bens_collected_issue_commits.csv'\n",
    "    bens_collected_commits = pd.read_csv(\n",
    "        file_path,\n",
    "    )\n",
    "    return bens_collected_commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ngkir_commits():\n",
    "    dtypes = {\n",
    "        'commit_sha': 'object',\n",
    "        'issue_id': 'float64',\n",
    "        'repo_name': 'object',\n",
    "        'url': 'object',\n",
    "        'html_url': 'object',\n",
    "        'message': 'object',\n",
    "        'author_login': 'object',\n",
    "        'author_type': 'object',\n",
    "        'committer_login': 'object',\n",
    "        'committer_type': 'object',\n",
    "        'stats_total': 'int64',\n",
    "        'stats_additions': 'int64',\n",
    "        'stats_deletions': 'int64',\n",
    "        'file_name': 'object',\n",
    "        'file_status': 'object',\n",
    "        'file_additions': 'int64',\n",
    "        'file_deletions': 'int64',\n",
    "        'file_changes': 'int64',\n",
    "        'file_patch': 'object',\n",
    "    }\n",
    "    file_path = f'{PROJECT_ROOT}/{CSV_FOLDER}/ngkir_bens_collected_issue_commits.csv'\n",
    "    commits = pd.read_csv(\n",
    "        file_path,\n",
    "        dtype=dtypes,\n",
    "    ).drop(columns=['issue_id'])\n",
    "    return commits\n",
    "\n",
    "def load_commit_event_relationships():\n",
    "    dtypes = {\n",
    "        'event_id': 'int64',\n",
    "        'issue_id': 'int64',\n",
    "        'repo_name': 'category',\n",
    "        'event_type': 'category',\n",
    "        'commit_id': 'object',\n",
    "        'commit_url': 'object',\n",
    "    }\n",
    "    file_path = f'{PROJECT_ROOT}/{CSV_FOLDER}/non_gkirbbi_issue_commit_events.csv'\n",
    "    renaming = {\n",
    "        'commit_id': 'commit_sha'\n",
    "    }\n",
    "    commit_event_rels = pd.read_csv(\n",
    "        file_path,\n",
    "        dtype=dtypes,\n",
    "    ).rename(columns=renaming)\n",
    "    return commit_event_rels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# Helper functions\n",
    "###################\n",
    "def get_issue(issues, id):\n",
    "    return issues[issues['issue_id'] == id]\n",
    "\n",
    "def get_issue_url(issues, id):\n",
    "    return issues[issues['issue_id'] == id]['issue_url']\n",
    "\n",
    "def get_comment(comments, id):\n",
    "    return comments[comments['comment_id'] == id]\n",
    "\n",
    "def get_comments_for_issue(comments, id):\n",
    "    return comments[comments['comment_issue_id'] == id]\n",
    "\n",
    "def get_event(events, id):\n",
    "    return events[events['event_id'] == id]\n",
    "\n",
    "def calculate_percent(numer, denom):\n",
    "    return round((numer/denom)*100, 2)\n",
    "\n",
    "def get_update_type(prev_ver, new_ver):\n",
    "    if pd.isnull(prev_ver) or pd.isnull(new_ver):\n",
    "        return NA_RELEASE_TYPE\n",
    "    try:\n",
    "        prev_split = prev_ver.split('.')\n",
    "        new_split = new_ver.split('.')\n",
    "        if int(new_split[0]) > int(prev_split[0]):\n",
    "            return MAJOR\n",
    "        elif int(new_split[1]) > int(prev_split[1]):\n",
    "            return MINOR\n",
    "        elif int(new_split[2]) > int(prev_split[2]):\n",
    "            return PATCH\n",
    "        else:\n",
    "            return NA_RELEASE_TYPE\n",
    "    except Exception as e:\n",
    "        return NA_RELEASE_TYPE\n",
    "    \n",
    "def get_update_type_v2(prev_ver, new_ver):\n",
    "    if pd.isnull(prev_ver) or pd.isnull(new_ver):\n",
    "        return NA_RELEASE_TYPE\n",
    "    try:\n",
    "        prev_split = prev_ver.split('.')\n",
    "        prev_major = prev_split[0]\n",
    "        prev_minor = prev_split[1]\n",
    "        prev_patch = prev_split[2]\n",
    "        new_split = new_ver.split('.')\n",
    "        new_major = new_split[0]\n",
    "        new_minor = new_split[1]\n",
    "        new_patch = new_split[2]\n",
    "        if new_major != prev_major:\n",
    "            return MAJOR\n",
    "        elif new_minor != prev_minor:\n",
    "            return MINOR\n",
    "        elif new_patch != prev_patch:\n",
    "            return PATCH\n",
    "        else:\n",
    "            return NA_RELEASE_TYPE\n",
    "    except Exception as e:\n",
    "        return NA_RELEASE_TYPE\n",
    "    \n",
    "def get_issue_id_for_commit(events, issues, commit):\n",
    "    try:\n",
    "        event_id = commit.commit_event_id\n",
    "        event = get_event(events, event_id)\n",
    "        if event.size == 0:\n",
    "            return np.nan\n",
    "        issue = get_issue(issues, event.event_issue_id.values[0].astype(np.int64))\n",
    "        if issue.size == 0:\n",
    "            return np.nan\n",
    "        return issue.issue_id.values[0].astype(np.int64)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def strfdelta(tdelta, fmt):\n",
    "    d = {\"days\": tdelta.days}\n",
    "    d[\"hours\"], rem = divmod(tdelta.seconds, 3600)\n",
    "    d[\"minutes\"], d[\"seconds\"] = divmod(rem, 60)\n",
    "    return fmt.format(**d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "def tfns(df, cols, title='', should_round=True, verbose=True):\n",
    "    table_five_number_summary(df, cols, title, should_round, verbose)\n",
    "\n",
    "def table_five_number_summary(df, cols, title='', should_round=True, verbose=True):\n",
    "    for_table = list()\n",
    "    for col in cols:\n",
    "        for_table.append(five_number_summary_for_table(col, df[col], should_round))\n",
    "    h = [title, 'Count', 'Mean', 'STD', 'Min', '25%', 'Median', '75%', 'Max']\n",
    "    print(tabulate(for_table, headers=h))\n",
    "    print()\n",
    "\n",
    "def five_number_summary_for_table(title, s, should_round=True, verbose=True):\n",
    "    c = s.describe()[0]\n",
    "    mean = my_round(s.describe()[1], 3) if should_round else s.describe()[1]\n",
    "    std = my_round(s.describe()[2], 3) if should_round else s.describe()[2]\n",
    "    minimum = my_round(s.describe()[3], 3) if should_round else s.describe()[3]\n",
    "    first_q = my_round(s.describe()[4], 3) if should_round else s.describe()[4]\n",
    "    median = my_round(s.describe()[5], 3) if should_round else s.describe()[5]\n",
    "    third_q = my_round(s.describe()[6], 3) if should_round else s.describe()[6]\n",
    "    maximum = my_round(s.describe()[7], 3) if should_round else s.describe()[7]\n",
    "    return [title, c, mean, std, minimum, first_q, median, third_q, maximum]\n",
    "\n",
    "\n",
    "def my_round(val, prec):\n",
    "    if type(val) == pd._libs.tslibs.timedeltas.Timedelta:\n",
    "        return val.round('1s')\n",
    "    else:\n",
    "        return round(val, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_percent(numer, denom):\n",
    "    return round((numer/denom)*100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAYSCALE_CMAP = sns.cubehelix_palette(\n",
    "    50,\n",
    "    hue=0.05,\n",
    "    rot=0,\n",
    "    light=0.9,\n",
    "    dark=0,\n",
    "    as_cmap=True\n",
    ")\n",
    "\n",
    "def create_hist_plot(params, ax, cbar_ax=None, color=GK_GREEN, bins=BINS_COUNT):\n",
    "    result = sns.histplot(\n",
    "        **params,\n",
    "        ax=ax,\n",
    "        kde=True,\n",
    "        bins=bins,\n",
    "        color=color,\n",
    "        cbar=cbar_ax is not None,\n",
    "        cbar_ax=cbar_ax\n",
    "    )\n",
    "    return result\n",
    "\n",
    "def create_reg_line(params, ax):\n",
    "    result = sns.regplot(\n",
    "        **params,\n",
    "        scatter=False,\n",
    "        line_kws={\"color\": GK_PURPLE},\n",
    "        ax=ax,\n",
    "    )\n",
    "    return result\n",
    "\n",
    "def save_plot(plot, full_file_path, dpi=400):\n",
    "    plot.savefig(f'{full_file_path}', dpi=400)\n",
    "    \n",
    "def save_fig(fig_id, tight_layout=True, fig_extension='png', resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + '.' + fig_extension)\n",
    "    print('Saving figure', fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "    \n",
    "def show_values_on_bars(axs, h_v=\"v\", h_offset=0, v_offset=0, val_formatter=int):\n",
    "    def _show_on_single_plot(ax):\n",
    "        if h_v == 'v':\n",
    "            for p in ax.patches:\n",
    "                _x = p.get_x() + p.get_width() / 2\n",
    "                _y = p.get_y() + p.get_height()\n",
    "                value = p.get_height()\n",
    "                ax.text(_x, _y, value, ha='center')\n",
    "        elif h_v == 'h':\n",
    "            for p in ax.patches:\n",
    "                _x = p.get_x() + p.get_width() + float(h_offset)\n",
    "                _y = p.get_y() + p.get_height() / 2 + float(v_offset)\n",
    "                value = val_formatter(p.get_width())\n",
    "                ax.text(_x, _y, value, ha='center')\n",
    "    if isinstance(axs, np.ndarray):\n",
    "        for idx, ax in np.ndenumerate(axs):\n",
    "            _show_on_single_plot(ax)\n",
    "    else:\n",
    "        _show_on_single_plot(axs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>FuncFormatters</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_log_exp(value):\n",
    "    table = ['\\u2070', '\\u2071', '\\u00B2', '\\u00B3', '\\u2074', '\\u2075', '\\u2076', '\\u2077', '\\u2078', '\\u2079']\n",
    "    if value >= 0:\n",
    "        return f'$10{table[int(value)]}$'\n",
    "    else:\n",
    "        return f'$10\\u207B{table[abs(int(value))]}$'\n",
    "\n",
    "log10_func_formatter = FuncFormatter(lambda x, pos: format_log_exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion_func_formatter = FuncFormatter(lambda x, pos: f'{int(x*100)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_outliers(df=None, col=None, threshold=3):\n",
    "    if col is None:\n",
    "        return filter_outliers_dataframe(df, threshold)\n",
    "    else:\n",
    "        return filter_outliers_series(df, col, threshold)\n",
    "\n",
    "def filter_outliers_dataframe(df, threshold=3):\n",
    "    z_scores = zscore(df, nan_policy='omit')\n",
    "    filtered_entries = (np.abs(z_scores) < threshold).all(axis=1)\n",
    "    return df[filtered_entries].copy()\n",
    "\n",
    "def filter_outliers_series(df, col, threshold=3):\n",
    "    z_scores = zscore(df[col], nan_policy='omit')\n",
    "    filtered_entries = (np.abs(z_scores) < threshold)\n",
    "    return df[filtered_entries].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mannwhitneyu_cliffsdelta(x, y):\n",
    "    # Use mann-whitney test because wilcoxon test requires same length\n",
    "    (statistic, pvalue) = mannwhitneyu(x, y)\n",
    "    (delta, size) = cliffsDelta(x, y)\n",
    "    print(f'''\\\n",
    "\\tMann-Whitney: statistic={statistic} pvalue={pvalue}\n",
    "\\tCliff'a Delta: delta={delta} size={size}\n",
    "    ''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
